<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Feature Engineering - Code2Data</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css">
</head>
<body>
    <nav class="navbar">
        <div class="logo">
            <a href="../index.html">Code2Data</a>
        </div>
        <ul class="nav-links">
            <li><a href="../machine-learning.html">Back to ML</a></li>
            <li><a href="feature-scaling.html">Previous: Feature Scaling</a></li>
            <li><a href="categorical-encoding.html">Next: Categorical Encoding</a></li>
        </ul>
    </nav>

    <main class="container">
        <article class="preprocessing-content">
            <h1><i class="fas fa-tools"></i> Feature Engineering</h1>
            
            <section class="intro">
                <p>Feature engineering is the process of creating new features from existing data to improve model performance. It's often considered the most important step in machine learning, requiring both domain knowledge and technical expertise.</p>
            </section>

            <section id="numerical-features">
                <h2>Numerical Feature Engineering</h2>
                
                <div class="subsection">
                    <h3>Mathematical Transformations</h3>
                    <div class="code-example">
                        <pre><code class="language-python">
import numpy as np
import pandas as pd

# Sample dataset
data = pd.DataFrame({
    'price': [100, 200, 300, 400, 500],
    'area': [1500, 2000, 2500, 3000, 3500]
})

# Log transformation
data['log_price'] = np.log1p(data['price'])

# Square root
data['sqrt_area'] = np.sqrt(data['area'])

# Polynomial features
data['area_squared'] = data['area'] ** 2

# Interaction features
data['price_per_sqft'] = data['price'] / data['area']

print("Transformed Features:\n", data)
                        </code></pre>
                    </div>
                </div>

                <div class="subsection">
                    <h3>Binning/Discretization</h3>
                    <div class="code-example">
                        <pre><code class="language-python">
# Equal-width binning
data['price_bins'] = pd.cut(data['price'], bins=3, labels=['low', 'medium', 'high'])

# Equal-frequency binning
data['area_quantiles'] = pd.qcut(data['area'], q=3, labels=['small', 'medium', 'large'])

# Custom binning
def age_category(age):
    if age < 18: return 'minor'
    elif age < 65: return 'adult'
    else: return 'senior'

data['age_category'] = data['age'].apply(age_category)
                        </code></pre>
                    </div>
                </div>
            </section>

            <section id="temporal-features">
                <h2>Temporal Feature Engineering</h2>
                <div class="code-example">
                    <pre><code class="language-python">
# Sample datetime data
dates_df = pd.DataFrame({
    'date': pd.date_range(start='2023-01-01', periods=5, freq='D'),
    'value': [100, 200, 150, 300, 250]
})

# Extract datetime components
dates_df['year'] = dates_df['date'].dt.year
dates_df['month'] = dates_df['date'].dt.month
dates_df['day'] = dates_df['date'].dt.day
dates_df['day_of_week'] = dates_df['date'].dt.dayofweek
dates_df['is_weekend'] = dates_df['day_of_week'].isin([5, 6])

# Time-based features
dates_df['days_since_start'] = (dates_df['date'] - dates_df['date'].min()).dt.days
dates_df['rolling_mean'] = dates_df['value'].rolling(window=3).mean()

print("Temporal Features:\n", dates_df)
                    </code></pre>
                </div>
            </section>

            <section id="text-features">
                <h2>Text Feature Engineering</h2>
                <div class="code-example">
                    <pre><code class="language-python">
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import re

# Sample text data
texts = [
    "This is a good example",
    "Another good example here",
    "Third example with different words"
]

# Basic text features
def extract_text_features(text):
    return {
        'char_count': len(text),
        'word_count': len(text.split()),
        'avg_word_length': np.mean([len(word) for word in text.split()]),
        'special_char_count': len(re.findall(r'[^a-zA-Z\s]', text))
    }

# Bag of Words
vectorizer = CountVectorizer()
bow_matrix = vectorizer.fit_transform(texts)
bow_df = pd.DataFrame(bow_matrix.toarray(), 
                     columns=vectorizer.get_feature_names_out())

# TF-IDF
tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(texts)
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), 
                       columns=tfidf.get_feature_names_out())

print("Bag of Words:\n", bow_df)
print("\nTF-IDF:\n", tfidf_df)
                    </code></pre>
                </div>
            </section>

            <section id="categorical-features">
                <h2>Categorical Feature Engineering</h2>
                <div class="code-example">
                    <pre><code class="language-python">
# Frequency encoding
def frequency_encode(df, column):
    freq_encoding = df[column].value_counts(normalize=True)
    return df[column].map(freq_encoding)

# Target encoding
def target_encode(df, column, target, k=5):
    global_mean = df[target].mean()
    agg = df.groupby(column)[target].agg(['count', 'mean'])
    counts = agg['count']
    means = agg['mean']
    
    # Smooth the mean
    smooth = (counts * means + k * global_mean) / (counts + k)
    return df[column].map(smooth)

# Example usage
data = pd.DataFrame({
    'category': ['A', 'B', 'A', 'C', 'B'],
    'target': [1, 0, 1, 0, 1]
})

data['category_freq'] = frequency_encode(data, 'category')
data['category_target'] = target_encode(data, 'category', 'target')

print("Encoded Features:\n", data)
                    </code></pre>
                </div>
            </section>

            <section id="advanced-techniques">
                <h2>Advanced Feature Engineering Techniques</h2>
                <div class="code-example">
                    <pre><code class="language-python">
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures

# Principal Component Analysis
def create_pca_features(X, n_components=2):
    pca = PCA(n_components=n_components)
    pca_features = pca.fit_transform(X)
    return pd.DataFrame(
        pca_features,
        columns=[f'pca_{i+1}' for i in range(n_components)]
    )

# Polynomial Features
def create_polynomial_features(X, degree=2):
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    poly_features = poly.fit_transform(X)
    feature_names = poly.get_feature_names_out(X.columns)
    return pd.DataFrame(poly_features, columns=feature_names)

# Example usage
X = pd.DataFrame({
    'feature1': [1, 2, 3, 4, 5],
    'feature2': [2, 4, 6, 8, 10]
})

pca_df = create_pca_features(X)
poly_df = create_polynomial_features(X)

print("PCA Features:\n", pca_df)
print("\nPolynomial Features:\n", poly_df)
                    </code></pre>
                </div>
            </section>

            <section id="feature-selection">
                <h2>Feature Selection Methods</h2>
                <div class="code-example">
                    <pre><code class="language-python">
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier

# Statistical selection
def select_features_statistical(X, y, k=5):
    selector = SelectKBest(score_func=f_classif, k=k)
    selected_features = selector.fit_transform(X, y)
    selected_columns = X.columns[selector.get_support()].tolist()
    return X[selected_columns]

# Feature importance using Random Forest
def select_features_importance(X, y, threshold=0.05):
    rf = RandomForestClassifier()
    rf.fit(X, y)
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    })
    return importance_df[importance_df['importance'] > threshold]

# Example usage
X = pd.DataFrame(np.random.rand(100, 5), columns=[f'feature_{i}' for i in range(5)])
y = np.random.randint(0, 2, 100)

statistical_features = select_features_statistical(X, y)
importance_features = select_features_importance(X, y)

print("Statistical Selection:\n", statistical_features.head())
print("\nFeature Importance:\n", importance_features)
                    </code></pre>
                </div>
            </section>
        </article>
    </main>

    <footer>
        <p>&copy; 2025 Code2Data. All rights reserved.</p>
    </footer>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>
</body>
</html>